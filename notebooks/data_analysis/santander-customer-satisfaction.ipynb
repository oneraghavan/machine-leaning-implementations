{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python2.7/site-packages')\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = \"/Users/raghavan/Downloads/santander-customer-satisfaction/train.csv\"\n",
    "\n",
    "test_file = \"/Users/raghavan/Downloads/santander-customer-satisfaction/test.csv\"\n",
    "\n",
    "\n",
    "# comma delimited is the default\n",
    "training_df = pd.read_csv(training_file, header = 0)\n",
    "test_df = pd.read_csv(test_file, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.swarmplot(training_df['TARGET'],training_df[\"var3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") #Needed to save figures\n",
    "from sklearn import cross_validation\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "training = training_df\n",
    "test = test_df\n",
    "\n",
    "# print(training.shape)\n",
    "# print(test.shape)\n",
    "\n",
    "# Replace -999999 in var3 column with most common value 2 \n",
    "# See https://www.kaggle.com/cast42/santander-customer-satisfaction/debugging-var3-999999\n",
    "# for details\n",
    "training = training.replace(-999999,2)\n",
    "\n",
    "\n",
    "# Replace 9999999999 with NaN\n",
    "# See https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/111360#post111360\n",
    "# training = training.replace(9999999999, np.nan)\n",
    "# training.dropna(inplace=True)\n",
    "# Leads to validation_0-auc:0.839577\n",
    "\n",
    "X = training.iloc[:,:-1]\n",
    "y = training.TARGET\n",
    "\n",
    "# Add zeros per row as extra feature\n",
    "X['n0'] = (X == 0).sum(axis=1)\n",
    "# # Add log of var38\n",
    "# X['logvar38'] = X['var38'].map(np.log1p)\n",
    "# # Encode var36 as category\n",
    "# X['var36'] = X['var36'].astype('category')\n",
    "# X = pd.get_dummies(X)\n",
    "\n",
    "# Add PCA components as features\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_normalized = normalize(X, axis=0)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_normalized)\n",
    "X['PCA1'] = X_pca[:,0]\n",
    "X['PCA2'] = X_pca[:,1]\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif,chi2\n",
    "from sklearn.preprocessing import Binarizer, scale\n",
    "\n",
    "p = 86 # 308 features validation_1-auc:0.848039\n",
    "p = 80 # 284 features validation_1-auc:0.848414\n",
    "p = 77 # 267 features validation_1-auc:0.848000\n",
    "p = 75 # 261 features validation_1-auc:0.848642\n",
    "# p = 73 # 257 features validation_1-auc:0.848338\n",
    "# p = 70 # 259 features validation_1-auc:0.848588\n",
    "# p = 69 # 238 features validation_1-auc:0.848547\n",
    "# p = 67 # 247 features validation_1-auc:0.847925\n",
    "# p = 65 # 240 features validation_1-auc:0.846769\n",
    "# p = 60 # 222 features validation_1-auc:0.848581\n",
    "\n",
    "X_bin = Binarizer().fit_transform(scale(X))\n",
    "selectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, y)\n",
    "selectF_classif = SelectPercentile(f_classif, percentile=p).fit(X, y)\n",
    "\n",
    "chi2_selected = selectChi2.get_support()\n",
    "chi2_selected_features = [ f for i,f in enumerate(X.columns) if chi2_selected[i]]\n",
    "print('Chi2 selected {} features {}.'.format(chi2_selected.sum(),\n",
    "   chi2_selected_features))\n",
    "f_classif_selected = selectF_classif.get_support()\n",
    "f_classif_selected_features = [ f for i,f in enumerate(X.columns) if f_classif_selected[i]]\n",
    "print('F_classif selected {} features {}.'.format(f_classif_selected.sum(),\n",
    "   f_classif_selected_features))\n",
    "selected = chi2_selected & f_classif_selected\n",
    "print('Chi2 & F_classif selected {} features'.format(selected.sum()))\n",
    "features = [ f for f,s in zip(X.columns, selected) if s]\n",
    "print (features)\n",
    "\n",
    "X_sel = X[features]\n",
    "\n",
    "print y.shape\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_sel, y, random_state=1301, stratify=y, test_size=0.4)\n",
    "\n",
    "# xgboost parameter tuning with p = 75\n",
    "# recipe: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19083/best-practices-for-parameter-tuning-on-models/108783#post108783\n",
    "\n",
    "ratio = float(np.sum(y == 1)) / np.sum(y==0)\n",
    "# Initial parameters for the parameter exploration\n",
    "# clf = xgb.XGBClassifier(missing=9999999999,\n",
    "#                 max_depth = 10,\n",
    "#                 n_estimators=1000,\n",
    "#                 learning_rate=0.1, \n",
    "#                 nthread=4,\n",
    "#                 subsample=1.0,\n",
    "#                 colsample_bytree=0.5,\n",
    "#                 min_child_weight = 5,\n",
    "#                 scale_pos_weight = ratio,\n",
    "#                 seed=4242)\n",
    "\n",
    "# gives : validation_1-auc:0.845644\n",
    "# max_depth=8 -> validation_1-auc:0.846341\n",
    "# max_depth=6 -> validation_1-auc:0.845738\n",
    "# max_depth=7 -> validation_1-auc:0.846504\n",
    "# subsample=0.8 -> validation_1-auc:0.844440\n",
    "# subsample=0.9 -> validation_1-auc:0.844746\n",
    "# subsample=1.0,  min_child_weight=8 -> validation_1-auc:0.843393\n",
    "# min_child_weight=3 -> validation_1-auc:0.848534\n",
    "# min_child_weight=1 -> validation_1-auc:0.846311\n",
    "# min_child_weight=4 -> validation_1-auc:0.847994\n",
    "# min_child_weight=2 -> validation_1-auc:0.847934\n",
    "# min_child_weight=3, colsample_bytree=0.3 -> validation_1-auc:0.847498\n",
    "# colsample_bytree=0.7 -> validation_1-auc:0.846984\n",
    "# colsample_bytree=0.6 -> validation_1-auc:0.847856\n",
    "# colsample_bytree=0.5, learning_rate=0.05 -> validation_1-auc:0.847347\n",
    "# max_depth=8 -> validation_1-auc:0.847352\n",
    "# learning_rate = 0.07 -> validation_1-auc:0.847432\n",
    "# learning_rate = 0.2 -> validation_1-auc:0.846444\n",
    "# learning_rate = 0.15 -> validation_1-auc:0.846889\n",
    "# learning_rate = 0.09 -> validation_1-auc:0.846680\n",
    "# learning_rate = 0.1 -> validation_1-auc:0.847432\n",
    "# max_depth=7 -> validation_1-auc:0.848534\n",
    "# learning_rate = 0.05 -> validation_1-auc:0.847347\n",
    "# \n",
    "\n",
    "clf = xgb.XGBClassifier(missing=9999999999,\n",
    "                max_depth = 5,\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.1, \n",
    "                nthread=4,\n",
    "                subsample=1.0,\n",
    "                colsample_bytree=0.5,\n",
    "                min_child_weight = 3,\n",
    "                scale_pos_weight = ratio,\n",
    "                reg_alpha=0.03,\n",
    "                seed=1301)\n",
    "                \n",
    "clf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"auc\",\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "        \n",
    "print('Overall AUC:', roc_auc_score(y, clf.predict_proba(X_sel, ntree_limit=clf.best_iteration)[:,1]))\n",
    "\n",
    "test['n0'] = (test == 0).sum(axis=1)\n",
    "# test['logvar38'] = test['var38'].map(np.log1p)\n",
    "# # Encode var36 as category\n",
    "# test['var36'] = test['var36'].astype('category')\n",
    "# test = pd.get_dummies(test)\n",
    "test_normalized = normalize(test, axis=0)\n",
    "pca = PCA(n_components=2)\n",
    "test_pca = pca.fit_transform(test_normalized)\n",
    "test['PCA1'] = test_pca[:,0]\n",
    "test['PCA2'] = test_pca[:,1]\n",
    "sel_test = test[features]    \n",
    "y_pred = clf.predict_proba(sel_test, ntree_limit=clf.best_iteration)\n",
    "\n",
    "submission = pd.DataFrame({\"ID\":test.index, \"TARGET\":y_pred[:,1]})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "mapFeat = dict(zip([\"f\"+str(i) for i in range(len(features))],features))\n",
    "ts = pd.Series(clf.booster().get_fscore())\n",
    "#ts.index = ts.reset_index()['index'].map(mapFeat)\n",
    "ts.sort_values()[-15:].plot(kind=\"barh\", title=(\"features importance\"))\n",
    "\n",
    "featp = ts.sort_values()[-15:].plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "fig_featp = featp.get_figure()\n",
    "fig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
